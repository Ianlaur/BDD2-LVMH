"""
ELITE ML Training - Maximum Accuracy with Optimal Speed

Combines multiple strategies:
- Advanced feature engineering
- Ensemble methods (multiple models voting)
- Smart label generation
- Parallel processing
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple
from sklearn.ensemble import (
    RandomForestClassifier, 
    RandomForestRegressor,
    GradientBoostingClassifier,
    GradientBoostingRegressor,
    VotingClassifier,
    VotingRegressor
)
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, mean_squared_error, r2_score, roc_auc_score
import joblib
import json
from datetime import datetime
from scipy.sparse import hstack, csr_matrix
import time


class EliteMLTrainer:
    """
    Elite ML trainer - Maximum accuracy with optimal speed.
    """
    
    def __init__(self):
        """Initialize trainer."""
        self.purchase_model = None
        self.churn_model = None
        self.clv_model = None
        self.tfidf_vectorizer = None
        self.scaler = StandardScaler()
        
    def create_elite_features(
        self,
        concepts_df: pd.DataFrame
    ) -> Tuple[pd.DataFrame, List[str]]:
        """
        Create advanced features for maximum accuracy.
        """
        print("\nðŸŽ¯ Creating ELITE features...")
        start = time.time()
        
        concept_col = 'matched_alias' if 'matched_alias' in concepts_df.columns else 'concept'
        
        # Group by client
        client_concepts = concepts_df.groupby('client_id')[concept_col].apply(list).to_dict()
        client_ids = list(client_concepts.keys())
        
        # 1. TF-IDF (word importance)
        print("   [1/4] TF-IDF features...")
        concept_texts = [" ".join(concepts) for concepts in client_concepts.values()]
        
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=800,
            min_df=2,
            max_df=0.9,
            ngram_range=(1, 3),  # Unigrams, bigrams, trigrams
            sublinear_tf=True
        )
        tfidf_features = self.tfidf_vectorizer.fit_transform(concept_texts)
        print(f"       âœ“ {tfidf_features.shape[1]} TF-IDF features")
        
        # 2. Advanced statistical features
        print("   [2/4] Statistical features...")
        stats_features = []
        
        # Define concept categories
        luxury_terms = ['luxury', 'premium', 'designer', 'high-end', 'exclusive', 'bespoke']
        budget_terms = ['budget', 'price', 'cost', 'affordable', 'economical']
        intent_terms = ['interest', 'looking', 'want', 'seek', 'need', 'desire']
        hesitation_terms = ['maybe', 'unsure', 'hesitant', 'thinking', 'consider']
        allergy_terms = ['allerg', 'sensiti', 'intoleran', 'avoid']
        gift_terms = ['gift', 'present', 'occasion', 'birthday', 'anniversary', 'event']
        
        for concepts in client_concepts.values():
            text = " ".join(concepts).lower()
            
            stats = [
                # Basic counts
                len(concepts),
                len(set(concepts)),
                len(text),
                
                # Category counts
                sum(1 for c in concepts if any(t in c.lower() for t in luxury_terms)),
                sum(1 for c in concepts if any(t in c.lower() for t in budget_terms)),
                sum(1 for c in concepts if any(t in c.lower() for t in intent_terms)),
                sum(1 for c in concepts if any(t in c.lower() for t in hesitation_terms)),
                sum(1 for c in concepts if any(t in c.lower() for t in allergy_terms)),
                sum(1 for c in concepts if any(t in c.lower() for t in gift_terms)),
                
                # Ratios
                len(set(concepts)) / max(len(concepts), 1),  # Uniqueness ratio
                sum(1 for c in concepts if len(c) > 20) / max(len(concepts), 1),  # Long concept ratio
                
                # Binary flags
                1 if 'budget @0' in text or 'no limit' in text else 0,
                1 if any(t in text for t in luxury_terms) else 0,
                1 if any(t in text for t in intent_terms) else 0,
            ]
            stats_features.append(stats)
            
        stats_array = np.array(stats_features)
        stats_sparse = csr_matrix(stats_array)
        print(f"       âœ“ {stats_sparse.shape[1]} statistical features")
        
        # 3. Concept co-occurrence features (top patterns)
        print("   [3/4] Co-occurrence features...")
        
        # Find common concept pairs
        from collections import Counter
        all_pairs = []
        for concepts in client_concepts.values():
            unique_concepts = list(set(concepts))
            for i in range(len(unique_concepts)):
                for j in range(i+1, len(unique_concepts)):
                    pair = tuple(sorted([unique_concepts[i], unique_concepts[j]]))
                    all_pairs.append(pair)
                    
        top_pairs = [pair for pair, count in Counter(all_pairs).most_common(50)]
        
        cooccur_features = []
        for concepts in client_concepts.values():
            unique_concepts = set(concepts)
            cooccur = [
                1 if all(c in unique_concepts for c in pair) else 0
                for pair in top_pairs
            ]
            cooccur_features.append(cooccur)
            
        cooccur_array = np.array(cooccur_features)
        cooccur_sparse = csr_matrix(cooccur_array)
        print(f"       âœ“ {cooccur_sparse.shape[1]} co-occurrence features")
        
        # 4. Concept diversity features
        print("   [4/4] Diversity features...")
        from collections import Counter
        
        diversity_features = []
        for concepts in client_concepts.values():
            concept_counts = Counter(concepts)
            
            # Diversity metrics
            entropy = -sum((count/len(concepts)) * np.log(count/len(concepts)) 
                          for count in concept_counts.values())
            max_freq = max(concept_counts.values())
            
            div_stats = [
                entropy,  # Concept entropy
                max_freq,  # Most common concept frequency
                len([c for c, count in concept_counts.items() if count == 1]),  # Singleton concepts
                len([c for c, count in concept_counts.items() if count > 2]),  # Repeated concepts
            ]
            diversity_features.append(div_stats)
            
        diversity_array = np.array(diversity_features)
        diversity_sparse = csr_matrix(diversity_array)
        print(f"       âœ“ {diversity_sparse.shape[1]} diversity features")
        
        # Combine all features
        print("   [*] Combining features...")
        X_combined = hstack([tfidf_features, stats_sparse, cooccur_sparse, diversity_sparse])
        
        # Create DataFrame
        feature_df = pd.DataFrame(
            X_combined.toarray(),
            columns=[f'feat_{i}' for i in range(X_combined.shape[1])]
        )
        feature_df.insert(0, 'client_id', client_ids)
        
        elapsed = time.time() - start
        print(f"âœ… Created {X_combined.shape[1]} features in {elapsed:.2f}s")
        
        return feature_df, []
        
    def train_ensemble_classifier(
        self,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray,
        model_name: str
    ) -> Tuple[object, Dict]:
        """
        Train ensemble classifier with voting.
        """
        print(f"   Training {model_name} ensemble...")
        start = time.time()
        
        # Create ensemble of 3 different models
        rf = RandomForestClassifier(
            n_estimators=250,
            max_depth=20,
            min_samples_split=4,
            n_jobs=-1,
            random_state=42
        )
        
        gb = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42
        )
        
        lr = LogisticRegression(
            max_iter=1000,
            C=1.0,
            random_state=42,
            n_jobs=-1
        )
        
        # Voting ensemble (soft voting for probabilities)
        ensemble = VotingClassifier(
            estimators=[('rf', rf), ('gb', gb), ('lr', lr)],
            voting='soft',
            n_jobs=-1
        )
        
        ensemble.fit(X_train, y_train)
        
        train_time = time.time() - start
        
        # Evaluate
        y_pred = ensemble.predict(X_test)
        y_pred_proba = ensemble.predict_proba(X_test)
        
        accuracy = (y_pred == y_test).mean()
        
        # Calculate AUC-ROC
        try:
            auc = roc_auc_score(y_test, y_pred_proba[:, 1])
        except:
            auc = 0.0
            
        print(f"       âœ“ Trained in {train_time:.2f}s")
        print(f"       âœ“ Accuracy: {accuracy:.2%}, AUC-ROC: {auc:.3f}")
        
        return ensemble, {
            "accuracy": float(accuracy),
            "auc_roc": float(auc),
            "train_time": train_time
        }
        
    def train_ensemble_regressor(
        self,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray
    ) -> Tuple[object, Dict]:
        """
        Train ensemble regressor with voting.
        """
        print(f"   Training CLV ensemble...")
        start = time.time()
        
        # Create ensemble
        rf = RandomForestRegressor(
            n_estimators=250,
            max_depth=20,
            min_samples_split=4,
            n_jobs=-1,
            random_state=42
        )
        
        gb = GradientBoostingRegressor(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42
        )
        
        ridge = Ridge(alpha=1.0, random_state=42)
        
        ensemble = VotingRegressor(
            estimators=[('rf', rf), ('gb', gb), ('ridge', ridge)],
            n_jobs=-1
        )
        
        ensemble.fit(X_train, y_train)
        
        train_time = time.time() - start
        
        # Evaluate
        y_pred = ensemble.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        
        print(f"       âœ“ Trained in {train_time:.2f}s")
        print(f"       âœ“ RMSE: ${rmse:.2f}, RÂ²: {r2:.3f}")
        
        return ensemble, {
            "rmse": float(rmse),
            "r2_score": float(r2),
            "train_time": train_time
        }
        
    def train_all_models(
        self,
        features_df: pd.DataFrame,
        labels_df: pd.DataFrame
    ) -> Dict:
        """
        Train all models with ensemble approach.
        """
        print("\nðŸš€ Training ELITE models (Ensemble Voting)...")
        
        # Merge features with labels
        data = features_df.merge(labels_df, on='client_id')
        
        X = data.drop(['client_id', 'purchased', 'churned', 'lifetime_value'], axis=1)
        
        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        
        results = {}
        
        # 1. Purchase prediction
        print("\n[1/3] Purchase Prediction:")
        y_purchase = data['purchased']
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_purchase, test_size=0.2, random_state=42, stratify=y_purchase
        )
        self.purchase_model, results['purchase'] = self.train_ensemble_classifier(
            X_train, X_test, y_train, y_test, "Purchase"
        )
        
        # 2. Churn prediction
        print("\n[2/3] Churn Prediction:")
        y_churn = data['churned']
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_churn, test_size=0.2, random_state=42, stratify=y_churn
        )
        self.churn_model, results['churn'] = self.train_ensemble_classifier(
            X_train, X_test, y_train, y_test, "Churn"
        )
        
        # 3. CLV prediction
        print("\n[3/3] CLV Prediction:")
        y_clv = data['lifetime_value']
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_clv, test_size=0.2, random_state=42
        )
        self.clv_model, results['clv'] = self.train_ensemble_regressor(
            X_train, X_test, y_train, y_test
        )
        
        return results
        
    def save_models(self, output_dir: Path):
        """Save trained models."""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        joblib.dump(self.purchase_model, output_dir / "purchase_ensemble.pkl")
        joblib.dump(self.churn_model, output_dir / "churn_ensemble.pkl")
        joblib.dump(self.clv_model, output_dir / "clv_ensemble.pkl")
        joblib.dump(self.scaler, output_dir / "scaler_elite.pkl")
        joblib.dump(self.tfidf_vectorizer, output_dir / "tfidf_elite.pkl")
        
        print(f"\nâœ… Saved ensemble models to {output_dir}")


def generate_smart_labels(concepts_df: pd.DataFrame) -> pd.DataFrame:
    """
    Generate smarter synthetic labels with better patterns.
    """
    print("\nðŸ§  Generating SMART synthetic labels...")
    
    np.random.seed(42)
    
    concept_col = 'matched_alias' if 'matched_alias' in concepts_df.columns else 'concept'
    
    labels = []
    for client_id in concepts_df['client_id'].unique():
        concepts = concepts_df[concepts_df['client_id'] == client_id][concept_col].tolist()
        text = " ".join(concepts).lower()
        
        # Calculate purchase score
        score = 0.3  # Base
        
        # Strong positive signals
        if 'budget @0' in text or 'no limit' in text:
            score += 0.30
        if any(x in text for x in ['luxury', 'designer', 'premium', 'exclusive']):
            score += 0.25
        if any(x in text for x in ['gift', 'birthday', 'anniversary', 'special occasion']):
            score += 0.20
        if any(x in text for x in ['interested', 'looking for', 'wants', 'seeks', 'desire']):
            score += 0.15
        if any(x in text for x in ['urgent', 'soon', 'immediately', 'asap']):
            score += 0.10
            
        # Negative signals
        if any(x in text for x in ['hesitant', 'unsure', 'maybe', 'thinking about']):
            score -= 0.15
        if any(x in text for x in ['budget conscious', 'affordable', 'economical']):
            score -= 0.10
            
        score = np.clip(score, 0.1, 0.95)
        
        # Add some randomness but less than before
        score += np.random.normal(0, 0.05)
        score = np.clip(score, 0.05, 0.95)
        
        purchased = 1 if np.random.random() < score else 0
        churned = 1 if np.random.random() < (1 - score * 0.8) else 0  # Engaged clients less likely to churn
        
        # CLV calculation
        clv = 3000 + np.random.normal(0, 1500)
        if 'budget @0' in text:
            clv += 25000
        if any(x in text for x in ['luxury', 'premium', 'designer']):
            clv += 12000
        if any(x in text for x in ['gift', 'occasion']):
            clv += 5000
        clv = max(500, clv)
        
        labels.append({
            'client_id': client_id,
            'purchased': purchased,
            'churned': churned,
            'lifetime_value': clv
        })
        
    labels_df = pd.DataFrame(labels)
    
    print(f"   Purchase rate: {labels_df['purchased'].mean():.1%}")
    print(f"   Churn rate: {labels_df['churned'].mean():.1%}")
    print(f"   Avg CLV: ${labels_df['lifetime_value'].mean():.2f}")
    
    return labels_df


def main():
    """Run elite training."""
    print("="*80)
    print("ELITE ML TRAINING - Maximum Accuracy with Ensemble Voting")
    print("="*80)
    
    # Load concepts
    concepts_file = Path("data/outputs/note_concepts.csv")
    if not concepts_file.exists():
        print(f"âŒ File not found: {concepts_file}")
        return
        
    concepts_df = pd.read_csv(concepts_file)
    print(f"\nðŸ“Š Loaded {len(concepts_df)} concept matches from {len(concepts_df['client_id'].unique())} clients")
    
    # Initialize trainer
    trainer = EliteMLTrainer()
    
    # Create features
    total_start = time.time()
    features_df, _ = trainer.create_elite_features(concepts_df)
    
    # Generate labels
    labels_df = generate_smart_labels(concepts_df)
    
    # Train models
    results = trainer.train_all_models(features_df, labels_df)
    
    total_time = time.time() - total_start
    
    # Summary
    print("\n" + "="*80)
    print("ðŸ† ELITE TRAINING COMPLETE")
    print("="*80)
    
    print(f"\nâ±ï¸  Total Time: {total_time:.2f}s")
    
    print(f"\nðŸ“Š Final Results:")
    print(f"   Purchase Prediction:")
    print(f"      Accuracy: {results['purchase']['accuracy']:.2%}")
    print(f"      AUC-ROC: {results['purchase']['auc_roc']:.3f}")
    
    print(f"\n   Churn Prediction:")
    print(f"      Accuracy: {results['churn']['accuracy']:.2%}")
    print(f"      AUC-ROC: {results['churn']['auc_roc']:.3f}")
    
    print(f"\n   CLV Prediction:")
    print(f"      RMSE: ${results['clv']['rmse']:.2f}")
    print(f"      RÂ² Score: {results['clv']['r2_score']:.3f}")
    
    # Save models
    trainer.save_models(Path("models/elite_predictive"))
    
    print("\nðŸŽ¯ ELITE models ready for production!")


if __name__ == "__main__":
    main()
